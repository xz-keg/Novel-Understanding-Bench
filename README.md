# 1M Novel Understanding Bench(Personal)

personal benchmark on Evaluating Long(1 million) Context Understanding Using Long Novel Comprehension

Analyse novel(=new) novels. Problems are designed as soon as new chapters are released. Context Length=500-1000k.
Manual Evaluation for personal interest. Sometimes the answer does not match the default correct answer but the model includes more detailed conditions, this will be viewed as "correct".

Previously only Gemini has 1M context length, now deepseek also offers its 1M context model so now we can compare.

Also includes some agent-based methods. Although they currently perform worse than model-based methods due to overlap and missing memory between sessions. 

# Results: 

# Season 1: 20 problems. 

# Model-based methods:

Gemini 3.1 Pro: 15

Gemini 3 Pro: 12

Deepseek V4 Lite: 10

Gemini 3 Flash: 9

Gemini 2.5 Pro: 5

# Agent-based methods:

Manus: 1

Minimax 2.5 agent: 0

Kimi 2.5 Agent Swarm: 0




